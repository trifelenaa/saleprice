# -*- coding: utf-8 -*-
"""saleprice

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AF3sLCqZpNPbBbpzAifaq_LiXp75KW7Z

# Загрузка данных и библиотек

Загрузить данные можно отсюда: [*диск*](https://disk.yandex.ru/d/ZoXAMdHkWwU9Bw)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Read in data into a dataframe
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

# Display top of dataframe
train_data.head()

"""# Препроцессинг для обучающей и тестовой выборок"""

train_data.info()

test_data.info()

train_data.isnull().sum()

#построим графики распределений для количественных данных
X = train_data.drop(columns = ['Id'],axis=1)
num = X.select_dtypes(include = ['float64', 'int64'])
num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);

X_trval = X.drop(columns = ['SalePrice'],axis=1)
X_test = test_data.drop(columns = ['Id'],axis=1)

X_test.shape

X_trval.shape

#преобразуем категориальные данные
categorical_columns = X_trval.select_dtypes(include=['object']).columns
X_trval = pd.get_dummies(X_trval, columns=categorical_columns, drop_first=True)
X_test = pd.get_dummies(X_test, columns=categorical_columns, drop_first=True)

X_trval, X_test = X_trval.align(X_test, join='outer', axis=1, fill_value=0)

#вставим пропущенные данные методом KNNimputer
imputer = KNNImputer(n_neighbors=5)
X_trval = pd.DataFrame(imputer.fit_transform(X_trval), columns=X_trval.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

#стандартизируем данные
scaler = StandardScaler()
X_trval = pd.DataFrame(scaler.fit_transform(X_trval), columns=X_trval.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

y = train_data['SalePrice']

"""# Обучение и валидация"""

#разделим обучающую выборку на две части, одна из которых валидационная
X_train, X_val, Y_train, Y_val = train_test_split(X_trval,y, test_size=0.2, random_state=37)

#выберем модель регрессионного градиентного бустинга
model_boost = GradientBoostingRegressor()

#параметры для поиска по сетке
parameters_boost = {'loss':('squared_error', 'absolute_error', 'quantile'),
              'learning_rate':[0.1, 0.01, 1e-3, 0.15],
              'n_estimators':[100,150,300,500],
              'criterion':('friedman_mse', 'squared_error'),
                   'random_state':(30,35,42)}

#выполним поиск по сетке
model = GridSearchCV(model_boost, parameters_boost, verbose=1,n_jobs=-1,cv=3)
model.fit(X_train, Y_train);

print('Best hyperparameters for boosting are: '+ str(model.best_params_))

#с найденными лучшими параметрами заново обучим модель (это потребовалось чтобы построить график зависимости RMSE)
model_boost = GradientBoostingRegressor(criterion= 'squared_error', learning_rate= 0.1, loss= 'squared_error', n_estimators= 300, random_state= 30)
model_boost.fit(X_train, Y_train)

val_predictions = model_boost.predict(X_val)

#посчитаем rmse
val_data_rmse = mean_squared_error(Y_val, val_predictions, squared = False)

print('RMSE of validation data for boosting : ', val_data_rmse)

"""# Визуализация"""

#построим графики зависимости RMSE от итерации для тренировочной и валидационной выборок
val_score = np.zeros((300,), dtype=np.float64)
train_score = np.zeros((300,), dtype=np.float64)

for i, y_pred in enumerate(model_boost.staged_predict(X_val)):
    val_score[i] = mean_squared_error(Y_val, y_pred, squared = False)

for i, y_pred in enumerate(model_boost.staged_predict(X_train)):
    train_score[i] = mean_squared_error(Y_train, y_pred, squared = False)

fig = plt.figure(figsize=(6, 6))
plt.subplot(1, 1, 1)
plt.title("RMSE")
plt.plot(
    np.arange(300) + 1,
    train_score,
    "b-",
    label="Training Set",
)
plt.plot(
    np.arange(300) + 1, val_score, "r-", label="Validation Set"
)
plt.legend(loc="upper right")
plt.xlabel("Boosting Iterations")
plt.ylabel("RMSE")
fig.tight_layout()
plt.show()

#важность признаков (первые семь)
feat_importances = pd.Series(model_boost.feature_importances_, index=X_trval.columns)
feat_importances.nlargest(7).plot(kind='barh')

"""# Предсказания и запись в файл"""

predictions = model.predict(X_test)

output = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predictions})
output.to_csv('submission.csv', index=False)